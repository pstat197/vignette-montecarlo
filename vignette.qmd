---
title: "PSTAT 197 Vignette Fall 2025"
author: "Phillip, Aakash, Aashish, Soome"
date: today
format: revealjs
revealjs:
  theme: simple
  slide-number: true
  transition: slide
---

## Monte Carlo Simulation: What is it?

Monte Carlo simulation is a computational technique that uses **repeated random sampling** to approximate unknown quantities. Instead of relying on closed-form analytical solutions, which may not exist or may be extremely complicated, Monte Carlo methods approximate expectations, integrals, probabilities, and distributions by simulating a large number of random draws and summarizing the results. The core idea is simple: *if you cannot compute something exactly, simulate it many times and approximate it.*

## Why Use Simulations: {.smaller}

Monte Carlo methods are widely used because:

-   Many statistical quantities (expectations, variances, probabilities) have no simple closed-form expressions.

-   Modern models (e.g., high-dimensional, nonlinear, or stochastic systems) are analytically intractable.

-   Simulation-based estimates converge to true values as the number of samples increases (by the Law of Large Numbers).

-   Monte Carlo methods provide natural tools for quantifying **uncertainty**, constructing confidence intervals, and evaluating sampling variability.

-   They are flexible and can be applied to almost any distribution or model, even when the underlying mathematics is complex.

In data science and applied statistics, Monte Carlo simulation provides a practical and powerful way to approximate results that traditional methods cannot handle easily.

## **Work Flow: Random Input → Sampling → Evaluate → Average**

A typical Monte Carlo simulation follows a simple flow:

1.  **Random Input:** Draw samples from a known distribution (e.g., normal, uniform).

2.  **Sampling:** Generate many independent random draws.

3.  **Evaluate Function:** Apply a function or rule to each sample (e.g., indicator for an event, transformation).

4.  **Average Output:** Compute the average of all values, which approximates the true expectation or probability of interest.

## Graphical Intuition:

```{r}
knitr::include_graphics("img/fig-MC-Workflow.jpg")
```


## Example 1: Estimating π Using Monte Carlo Simulation
---

### Context

Monte Carlo simulation can estimate mathematical constants that cannot be computed easily using simple geometry or closed-form calculus.  
Estimating π is one of the most classic Monte Carlo demonstrations because:

- It is visual and intuitive  
- It shows how repeated random sampling converges  
- It relies only on uniform random points  
- It illustrates the core idea: **approximate by simulating many random samples**

This method relies on the geometric relationship between a circle and its enclosing square.

---

### Setup (Geometry & Probability)

We consider:

- A square \([-1,1] \times [-1,1]\), area = 4  
- A circle of radius 1, area = \(\pi\)  

A uniformly drawn point from the square satisfies:

\[
P(\text{inside circle}) = \frac{\pi}{4}
\]

Thus the Monte Carlo estimator of π is:

\[
\hat{\pi} = 4 \cdot \frac{\#\{\text{points inside circle}\}}{n}.
\]

---

### Setup (Simulation Algorithm)

Simulation procedure:

1. Generate \(n\) random points \((x_i, y_i)\) uniformly on the square  
2. Check if each point satisfies \(x_i^2 + y_i^2 \le 1\)  
3. Count the number of points inside the circle  
4. Estimate π via  
   \[
   4 \cdot (\text{inside} / n)
   \]

This algorithm illustrates the Monte Carlo workflow:  
*Random input → Evaluate → Aggregate.*

---

### Visual Walk-Through

Scatterplot of sampled points  
(blue = inside circle, red = outside):

```{r}
knitr::include_graphics("img/fig-pi-scatter.png")
```

---

### Second Visual
Convergence of the Monte Carlo estimate of π
(The estimate stabilizes as n increases):

```{r}
knitr::include_graphics("img/fig-pi-convergence.png")
```

---

### Analysis

- Early estimates of π fluctuate a lot due to randomness.  
- As the sample size \(n\) grows, the estimates stabilize around the true value.  
- This behavior illustrates the Law of Large Numbers:
  \[
  \hat{\pi}_n \rightarrow \pi \quad \text{as } n \rightarrow \infty.
  \]

**Interpretation:**  
Each individual point is noisy, but the *average* reveals the true quantity. This is exactly how Monte Carlo methods approximate probabilities, expectations, and integrals. The same principle underlies more advanced algorithms such as MCMC and Gibbs sampling.

---

### Summary

Monte Carlo estimation of π shows how randomness can be used to approximate mathematical quantities. By repeatedly sampling points and averaging outcomes, the estimate converges to \(\pi\).

This illustrates the essential Monte Carlo workflow:

**simulate → evaluate → average → converge.**

---

### Example 2: Gibbs Sampling (Markov Chain Monte Carlo (MCMC) Method)

---

### Context

Gibbs Sampling is a practical application of a type Monte Carlo Simulation. It is used to find the joint density distribution when directly sampling is difficult. 

---

### Setup 

Consider the following situation:
Suppose we have a directed graph of four 2-bit vertices, $\vec{G} = \{V, \vec{E}\}$ Where $V = {a, b, c, d}$ and $\vec{E} = {(\vec{ab},\vec{ac} ,\vec{bd}, \vec{cd})}$.

```{r}
knitr::include_graphics("img/fig-simple-graph.png")
```

Additionally, each vertex can be in one of 2 states: +1 or -1 state. 


---

### Setup 

We are interested in the probability each vertex being in a any state, and we know the probability of a vertex being in any state given the states of its parent vertices:

$\mathbb{P}(+d | +b, +c) = 0.3$, 

$\mathbb{P}(+d | +b, -c) = 0.1$, 

$\mathbb{P}(+d | -b, +c) = 0.2$, 

$\mathbb{P}(+d | -b, -c) = 0.9$, 

$\mathbb{P}(+a) = 0.8$, 

$\mathbb{P}(+b | +a) = 0.8$, 

$\mathbb{P}(+b | -a) = 0.5$, 

$\mathbb{P}(+c | +a) = 0.7$, 

$\mathbb{P}(+c | -a) = 0.1$

With this information, we can use gibbs sampling to estimate the probabilities of each vertex being in each state.

---

## Visual Walk-Through of Gibbs Sampling

In **one round** of Gibbs sampling:

1. Select one vertex \(x\) to resample.  
2. Observe the states of all other vertices.  
3. Sample \(x\) based on its **conditional probability** given the current states of other vertices.

--- 

##  Visual Walk-Through of Gibbs Sampling

Current vertex states: `+a, -b, +c, +d`  

- Suppose we pick vertex `c`.  

- The conditional probability is: $\mathbb{P}(+c \mid +a) = 0.7$

- If the random number generator chooses `-c`, we update the vertices to `+a, -b, -c, +d`.  

Once all vertices are sampled once, we complete **one round**, which produces **one instance** of vertex states.

---

##  Visual Walk-Through of Gibbs Sampling

- Kick off this MCMC process by initializing `a, b, c, d` to any state.

- Next, run a Gibbs sampling round (explained on prev slide) on this state.

- Repeat that process **N times** (We Pick N), each time storing our results:
```{r}
knitr::include_graphics("img/gibbs-sample.png")
```

- After sampling, to estimate the probability of each vertex being +1:

$\mathbb{P}(+x) = \frac{\text{# of instances with } +x}{N}$

This allows us to estimate **probabilities** using only conditional distributions.

```{r}
knitr::include_graphics("img/gibbs-result.png")
```
---

### Analysis

- By Law of Large Numbers, our distribution will converge to true population as N approaches infinity. However, realistically we don't have time for infinity trials. 

- Remember that to start off Gibbs Sampling, we choose a random state configuration for the vertices to begin with. If we do not run a large number of trials, this random state and subsequent instances could hurt our estimate. To account for this, we can remove the results from first few iterations, called the "burn in period", when the simulation did not yet have time to adjust.

---

### Analysis

- Consider if we had to find the probabilities of each vertex for a much more complex graph with more vertices. Gibbs sampling makes it feasible to get a reasonable estimate reasonable limited knowledge and computational power using randomness.

---

### Real World Application: Ising Model

```{r}
knitr::include_graphics("img/fig-ising-model.png")
```


Ising model is a simple model in statistical physics for studying how particles or atoms “align” with each other.

In an Ising Model, we have a lattice of particles, each with either an up (+1) spin or down (-1) spin. Particles tend to align with their neighbors.


---

### Real World Application: Ising Model

```{r}
knitr::include_graphics("img/fig-ising-model.png")
```


As the lattice size increases, the number of possible configurations grow exponentially, making direct sampling from the distribution impossible.

Conditioning on the configuration of neighbors, Gibbs sampling can help us find the optimal configuration for all particles when given information about a particle's neighbors. 
