---
title: "PSTAT 197 Vignette Fall 2025"
author: "Phillip, Aakash, Aashish, Soome"
date: today
format: revealjs
revealjs:
  theme: simple
  slide-number: true
  transition: slide
---

## Monte Carlo Simulation: What is it?

Monte Carlo simulation is a computational technique that uses **repeated random sampling** to approximate unknown quantities. Instead of relying on closed-form analytical solutions, which may not exist or may be extremely complicated, Monte Carlo methods approximate expectations, integrals, probabilities, and distributions by simulating a large number of random draws and summarizing the results. The core idea is simple: *if you cannot compute something exactly, simulate it many times and approximate it.*

## Why Use Simulations: {.smaller}

Monte Carlo methods are widely used because:

-   Many statistical quantities (expectations, variances, probabilities) have no simple closed-form expressions.

-   Modern models (e.g., high-dimensional, nonlinear, or stochastic systems) are analytically intractable.

-   Simulation-based estimates converge to true values as the number of samples increases (by the Law of Large Numbers).

-   Monte Carlo methods provide natural tools for quantifying **uncertainty**, constructing confidence intervals, and evaluating sampling variability.

-   They are flexible and can be applied to almost any distribution or model, even when the underlying mathematics is complex.

In data science and applied statistics, Monte Carlo simulation provides a practical and powerful way to approximate results that traditional methods cannot handle easily.

## **Work Flow: Random Input → Sampling → Evaluate → Average**

A typical Monte Carlo simulation follows a simple flow:

1.  **Random Input:** Draw samples from a known distribution (e.g., normal, uniform).

2.  **Sampling:** Generate many independent random draws.

3.  **Evaluate Function:** Apply a function or rule to each sample (e.g., indicator for an event, transformation).

4.  **Average Output:** Compute the average of all values, which approximates the true expectation or probability of interest.

## Graphical Intuition:

![](MC Workflow.jpg)

## Example 1: Estimating π Using Monte Carlo Simulation
---

### Context

Monte Carlo simulation can estimate mathematical constants that cannot be computed easily using simple geometry or closed-form calculus.  
Estimating π is one of the most classic Monte Carlo demonstrations because:

- It is visual and intuitive  
- It shows how repeated random sampling converges  
- It relies only on uniform random points  
- It illustrates the core idea: **approximate by simulating many random samples**

This method relies on the geometric relationship between a circle and its enclosing square.

---

### Setup (Geometry & Probability)

We consider:

- A square \([-1,1] \times [-1,1]\), area = 4  
- A circle of radius 1, area = \(\pi\)  

A uniformly drawn point from the square satisfies:

\[
P(\text{inside circle}) = \frac{\pi}{4}
\]

Thus the Monte Carlo estimator of π is:

\[
\hat{\pi} = 4 \cdot \frac{\#\{\text{points inside circle}\}}{n}.
\]

---

### Setup (Simulation Algorithm)

Simulation procedure:

1. Generate \(n\) random points \((x_i, y_i)\) uniformly on the square  
2. Check if each point satisfies \(x_i^2 + y_i^2 \le 1\)  
3. Count the number of points inside the circle  
4. Estimate π via  
   \[
   4 \cdot (\text{inside} / n)
   \]

This algorithm illustrates the Monte Carlo workflow:  
*Random input → Evaluate → Aggregate.*

---

### Visual Walk-Through

Scatterplot of sampled points  
(blue = inside circle, red = outside):

```{r}
knitr::include_graphics("img/pi-scatter.png")
```

---

### Second Visual
Convergence of the Monte Carlo estimate of π
(The estimate stabilizes as n increases):

```{r}
knitr::include_graphics("img/pi-convergence.png")
```

---

### Analysis

- Early estimates of π fluctuate a lot due to randomness.  
- As the sample size \(n\) grows, the estimates stabilize around the true value.  
- This behavior illustrates the Law of Large Numbers:
  \[
  \hat{\pi}_n \rightarrow \pi \quad \text{as } n \rightarrow \infty.
  \]

**Interpretation:**  
Each individual point is noisy, but the *average* reveals the true quantity. This is exactly how Monte Carlo methods approximate probabilities, expectations, and integrals. The same principle underlies more advanced algorithms such as MCMC and Gibbs sampling.

---

### Summary

Monte Carlo estimation of π shows how randomness can be used to approximate mathematical quantities. By repeatedly sampling points and averaging outcomes, the estimate converges to \(\pi\).

This illustrates the essential Monte Carlo workflow:

**simulate → evaluate → average → converge.**
